{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Elmo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEVLBzZ3pzOw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-AmvDIIzS2w"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm0YbhvKzTBF"
      },
      "source": [
        "pip install 'h5py<3.0.0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8mhNt1sesQM"
      },
      "source": [
        "pip install tensorflow==1.15.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6QqdON1et24"
      },
      "source": [
        "pip install keras==2.5.0rc0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpH69jruevEv"
      },
      "source": [
        "!pip install wordninja"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6qVtamazQWp"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCHCGAvYzQcn"
      },
      "source": [
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "import wordninja as wn\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense,Lambda,Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX3Xw9MXgk2S"
      },
      "source": [
        "# Parametri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhT9kfmOo00G"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Cyber Security/Elmo/\"\n",
        "elmo_path = \"/content/drive/MyDrive/Cyber Security/Elmo/3\"\n",
        "\n",
        "batch_size = 32\n",
        "numEpochs = 10\n",
        "\n",
        "start_fold = 1\n",
        "end_fold = 11\n",
        "\n",
        "nfolds = 10\n",
        "\n",
        "nome_file = \"Dataset_Completo.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Md_C7LcsEL9"
      },
      "source": [
        "# Definizioni"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SieB4yCVzD7O"
      },
      "source": [
        "def arrayToSentence(x):\n",
        "  string=''\n",
        "  for a in x:\n",
        "    string=string + a + ' '\n",
        "  return string\n",
        "\n",
        "def buildDataset():\n",
        "  filecsv = open(path + nome_file, newline=\"\")\n",
        "  lettore = csv.reader(filecsv, delimiter=\";\")\n",
        "\n",
        "  dataset_x = []\n",
        "  dataset_y = []\n",
        "  temp_y = []\n",
        "  for a in lettore:\n",
        "    dataset_y.append(a[0])\n",
        "    if a[0] == 'dga':\n",
        "      temp_y.append(1)\n",
        "    else:\n",
        "      temp_y.append(0)\n",
        "    split = wn.split(a[3])\n",
        "    sen = arrayToSentence(split)\n",
        "    dataset_x.append(sen)\n",
        "\n",
        "  filecsv.close()\n",
        "\n",
        "  return dataset_x, dataset_y, temp_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa3XafjoSk4p"
      },
      "source": [
        "def kfold(x, y, temp_y):\n",
        "  # Divide the dataset into training + holdout and testing with folds\n",
        "  sss = StratifiedKFold(n_splits=nfolds)\n",
        "\n",
        "  fold = 0\n",
        "  for train, test in sss.split(x, temp_y):\n",
        "    print(\"Writing fold \" + str(fold + 1) + \" to csv...\")\n",
        "    fold += 1\n",
        "    x_train, x_test, y_train, y_test, y_temp_train, y_temp_test = x[train], x[test], y[train], y[test], temp_y[train], temp_y[test]\n",
        "    np.savetxt(path + \"Dataset/x_train\" + str(fold) + \".csv\", x_train, fmt='%s', delimiter=';')\n",
        "    np.savetxt(path + \"Dataset/x_test\" + str(fold) + \".csv\", x_test, fmt='%s', delimiter=';')\n",
        "    np.savetxt(path + \"Dataset/y_train\" + str(fold) + \".csv\", y_train, fmt='%s', delimiter=';')\n",
        "    np.savetxt(path + \"Dataset/y_test\" + str(fold) + \".csv\", y_test, fmt='%s', delimiter=';')\n",
        "    np.savetxt(path + \"Dataset/temp_y_train\" + str(fold) + \".csv\", y_temp_train, fmt='%i', delimiter=';')\n",
        "    np.savetxt(path + \"Dataset/temp_y_test\" + str(fold) + \".csv\", y_temp_test, fmt='%i', delimiter=';')\n",
        "  print(\"Files created\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeMs0jxwNXjX"
      },
      "source": [
        "def encode(le, labels):\n",
        "    enc = le.transform(labels)\n",
        "    return tf.keras.utils.to_categorical(enc)\n",
        "\n",
        "def decode(le, one_hot):\n",
        "    dec = np.argmax(one_hot, axis=1)\n",
        "    return le.inverse_transform(dec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK-mBbYlgqiW"
      },
      "source": [
        "# Preprocesing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLgkHmtRp-Hj"
      },
      "source": [
        "dataset_x, dataset_y, temp_y = buildDataset()\n",
        "dataset_x = np.array(dataset_x)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(dataset_y)\n",
        "\n",
        "dataset_y_encode = encode(le, dataset_y)\n",
        "dataset_y = np.array(dataset_y_encode)\n",
        "\n",
        "temp_y = np.array(temp_y)\n",
        "\n",
        "# kfold(dataset_x, dataset_y, temp_y) DA FARE SOLO LA PRIMA VOLTA PER GENERARE I VARI FOLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIxxcVXusLMn"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVZZfpNC0qlM"
      },
      "source": [
        "fold = 0\n",
        "for fold in range(start_fold, end_fold):\n",
        "  print('Fold: ', fold, 'Epochs: ', numEpochs)\n",
        "  #Get fold by csv\n",
        "  x_train = np.genfromtxt(path + \"Dataset/x_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n",
        "  x_test = np.genfromtxt(path + \"Dataset/x_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n",
        "  y_train = np.genfromtxt(path + \"Dataset/y_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n",
        "  y_test = np.genfromtxt(path + \"Dataset/y_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n",
        "  y_train_temp = np.genfromtxt(path + \"Dataset/temp_y_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n",
        "  y_test_temp = np.genfromtxt(path + \"Dataset/temp_y_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n",
        "\n",
        "  print('Model Construction...')\n",
        "  model = None\n",
        "\n",
        "  #Parte costruzione del modello\n",
        "  # importo il modulo con la funzione di embedding ELMo\n",
        "  elmo = hub.Module(elmo_path)\n",
        "\n",
        "  # Definisco la funzione di embedding\n",
        "  def ELMoEmbedding(x):\n",
        "    return elmo(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n",
        "\n",
        "  input_text = Input(shape=(1,), dtype=tf.string)\n",
        "  embedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n",
        "  dense = Dense(128, activation='relu')(embedding)\n",
        "  pred = Dense(len(y_train[0]), activation='sigmoid')(dense)\n",
        "  model = Model(inputs=[input_text], outputs=pred)\n",
        "\n",
        "  model.compile('adam', 'binary_crossentropy', metrics=['accuracy',\n",
        "      tf.keras.metrics.AUC(),\n",
        "      tf.keras.metrics.Precision(),\n",
        "      tf.keras.metrics.Recall()\n",
        "      ])\n",
        "\n",
        "  print('...done!')\n",
        "\n",
        "  print('Train...')\n",
        "  #parte di training\n",
        "  Type = 'binary-'\n",
        "  with tf.compat.v1.Session() as session:\n",
        "    earlystop = EarlyStopping(monitor='loss', patience=3)\n",
        "    best_save = ModelCheckpoint(path + 'Saved/bestmodel' + str(fold) + '.hdf5', save_best_only=True, \n",
        "                                save_weights_only=False, \n",
        "                                monitor='val_loss', \n",
        "                                mode='min')\n",
        "    tf.compat.v1.keras.backend.set_session(session)\n",
        "    session.run(tf.compat.v1.global_variables_initializer())\n",
        "    session.run(tf.compat.v1.tables_initializer())\n",
        "    history = model.fit(x_train,y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=numEpochs, \n",
        "              callbacks=[earlystop, best_save],\n",
        "              validation_split=0.1\n",
        "              )\n",
        "    print('\\nhistory dict:', history.history)\n",
        "\n",
        "  print('...done!')\n",
        "\n",
        "  print('Test...')\n",
        "  #Parte di test\n",
        "  with tf.compat.v1.Session() as session:\n",
        "    tf.compat.v1.keras.backend.set_session(session)\n",
        "    session.run(tf.compat.v1.global_variables_initializer())\n",
        "    session.run(tf.compat.v1.tables_initializer())\n",
        "    best_model = load_model(path + 'Saved/bestmodel' + str(fold) + '.hdf5')\n",
        "    predicts = best_model.predict(x_test, batch_size=batch_size, verbose= 1)\n",
        "\n",
        "  # print(predicts)\n",
        "  y_preds = decode(le, predicts)\n",
        "  y_test_temp = np.where(y_test_temp == 1, 'dga', 'legit')\n",
        "  print('...done!')\n",
        "\n",
        "  print('Results:')\n",
        "  #Plotta i risultati\n",
        "  cm = metrics.confusion_matrix(y_test_temp, y_preds)\n",
        "  np.savetxt(path + 'Saved/confusion_matrix' + str(fold) + '.csv', cm, delimiter=',',  fmt='%i')\n",
        "  metrics1 = metrics.classification_report(y_test_temp, y_preds, output_dict=True, target_names=['legit', 'dga'])\n",
        "  print('Confusion_matrix:')\n",
        "  print(cm)\n",
        "  print('Classification_report:')\n",
        "  print(metrics1)\n",
        "  try:\n",
        "      df1 = pd.read_csv(path + \"Saved/metrics.csv\", index_col=[0])\n",
        "      df1 = df1.append(pd.DataFrame(metrics1))\n",
        "      df1.to_csv(path + \"Saved/metrics.csv\")\n",
        "  except:\n",
        "      pd.DataFrame(metrics1).to_csv(path + \"Saved/metrics.csv\")\n",
        "\n",
        "  df_cm = pd.DataFrame(cm, index = [i for i in le.classes_],\n",
        "                    columns = [i for i in le.classes_])\n",
        "  plt.figure(1, figsize = (10,7))\n",
        "  sn.heatmap(df_cm, annot=True, fmt=\"d\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}